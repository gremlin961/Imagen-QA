{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a331300-5ff3-4219-83c0-ce11703cc9e1",
   "metadata": {},
   "source": [
    "# How To Use Vertex Text-to-Image Generative AI To Inspect Image Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34429b82-078c-4107-827d-8aaed4ab1121",
   "metadata": {},
   "source": [
    "This notebook outlines how to interact with Vertex AI's Imagen GenAI models to inspect images and generate detailed information about its content. Visual Question Answering (VQA) lets you provide an image to the model and ask a question about the image's contents. In response to your question you get one or more natural language answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed54ec-9f36-40af-8d2e-c6fd32a76222",
   "metadata": {},
   "source": [
    "## Prepare the python development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ce888-8c52-476b-96d0-7f65cb259f8d",
   "metadata": {},
   "source": [
    "First, let's identify any project specific variables to customize this notebook to your GCP environment. Change YOUR_PROJECT_ID with your own GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205088b5-6d5b-46dd-9d77-a8352eac630b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"rkiles-demo-host-vpc\"\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75038c4-0365-4c03-a38c-21316392dad8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92738a-045b-48a4-ab0e-1d98fad2089a",
   "metadata": {},
   "source": [
    "Install any needed python modules from our requirements.txt file. Most Vertex Workbench environments include all the packages we'll be using, but if you are using an external Jupyter Notebook or require any additional packages for your own needs, you can simply add them to the included requirements.txt file an run the folloiwng commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69175b8-63c4-4b5a-837d-055743c0b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b2ce7-0277-4ada-9648-34c67774bfd2",
   "metadata": {},
   "source": [
    "Now we will import all required modules. For our purpose, we will be utilizing the following:\n",
    "\n",
    "- vertexai - Provides access to the Vertex AI modules\n",
    "- base64 - Imagen API requests return generated or edited images as base64-encoded strings. This module will help us decode this data to an image file\n",
    "- json - Python module used to interact with JSON data. Imagen returns results in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e15897-0cb0-4173-9b91-17270a10b989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.vision_models import ImageTextModel, Image\n",
    "from io import BytesIO\n",
    "import urllib\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b2668-962a-42da-b92b-4932f224bb0d",
   "metadata": {},
   "source": [
    "## Authenticate to the Vertex AI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57a2c4-d51a-42b8-b0c7-24aced49d64b",
   "metadata": {},
   "source": [
    "Specify the Project ID and location from the variables defined in the start of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7d8c7-90ad-44b1-951b-3970cf58c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "model = ImageTextModel.from_pretrained(\"imagetext@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d378e-2033-4ccd-95a8-1f2e5e70c038",
   "metadata": {},
   "source": [
    "Define the Q&A prompt that will be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f16782-6804-4ea6-b44b-00ea66c05710",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_prompt = \"does this sink have a dryingboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb510b4-da11-45ed-b082-0f011e366445",
   "metadata": {
    "tags": []
   },
   "source": [
    "The REST API expects a json payload containing the image data that will be inspected by the model. We are only passing a few of the available request options below, but you can find more information about the REST API and additional featuers here https://cloud.google.com/vertex-ai/docs/generative-ai/image/visual-question-answering\n",
    "\n",
    "Next we will create a base64 encoded byte string of the image we will be passing to the Imagen API. We'll start with a local file named sink1.jpg. You can change this to sink2.jpg and re-run the script to verify the model's prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07571cce-5bb3-46a7-a165-23935d002207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_image = Image.load_from_file(location='./sink1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dde75-4fbe-4b00-af27-299e501b934a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_url = ('https://mobileimages.lowes.com/productimages/79fc44bb-c9ef-453e-9000-d7cae1372431/49607177.jpg')\n",
    "source_image = urllib.urlopen(image_url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1980e70-eef9-49e7-a66e-e4c1bff5cd66",
   "metadata": {},
   "source": [
    "We will now create the request body that will be passed to the REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4152412-a31b-4fa2-aa0a-280f5115c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = model.ask_question(\n",
    "    image=source_image,\n",
    "    question=vqa_prompt\n",
    "    # Optional:\n",
    "    #number_of_results=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c44c6c-adcd-4418-b805-8be482304e1e",
   "metadata": {},
   "source": [
    "You can optionally uncomment the following to view the returned status code for verification or troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfcca9-20e4-477d-adfa-3cc66fc3c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e2d0b-8755-48c9-8d64-7a0ab8be5ac7",
   "metadata": {},
   "source": [
    "That's it! Congratulations on defining your first visual Q&A with Imagen!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
