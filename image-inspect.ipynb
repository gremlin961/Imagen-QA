{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a331300-5ff3-4219-83c0-ce11703cc9e1",
   "metadata": {},
   "source": [
    "# How To Use Vertex Text-to-Image Generative AI To Inspect Image Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34429b82-078c-4107-827d-8aaed4ab1121",
   "metadata": {},
   "source": [
    "This notebook outlines how to interact with Vertex AI's Imagen GenAI models to inspect images and generate detailed information about its content. Visual Question Answering (VQA) lets you provide an image to the model and ask a question about the image's contents. In response to your question you get one or more natural language answers.\n",
    "\n",
    "More information about vertexai vision_models API can be found at https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.vision_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed54ec-9f36-40af-8d2e-c6fd32a76222",
   "metadata": {},
   "source": [
    "## Prepare the python development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ce888-8c52-476b-96d0-7f65cb259f8d",
   "metadata": {},
   "source": [
    "First, let's identify any project specific variables to customize this notebook to your GCP environment. Change YOUR_PROJECT_ID with your own GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205088b5-6d5b-46dd-9d77-a8352eac630b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"YOUR_PROJECT_ID\"\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92738a-045b-48a4-ab0e-1d98fad2089a",
   "metadata": {},
   "source": [
    "Install any needed python modules from our requirements.txt file. Most Vertex Workbench environments include all the packages we'll be using, but if you are using an external Jupyter Notebook or require any additional packages for your own needs, you can simply add them to the included requirements.txt file an run the folloiwng commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69175b8-63c4-4b5a-837d-055743c0b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b2ce7-0277-4ada-9648-34c67774bfd2",
   "metadata": {},
   "source": [
    "Now we will import all required modules. For our purpose, we will be utilizing the following:\n",
    "\n",
    "- vertexai - Provides access to the Vertex AI modules\n",
    "- urllib - Download an image from a url to pass to the model for Q&A\n",
    "- os - Remove the downloaded file once the Image method generates the encoded base64 string to send to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e15897-0cb0-4173-9b91-17270a10b989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.vision_models import ImageTextModel, Image\n",
    "import urllib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b2668-962a-42da-b92b-4932f224bb0d",
   "metadata": {},
   "source": [
    "## Connect to the Vertex AI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57a2c4-d51a-42b8-b0c7-24aced49d64b",
   "metadata": {},
   "source": [
    "Specify the Project ID and location from the variables defined in the start of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7d8c7-90ad-44b1-951b-3970cf58c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "model = ImageTextModel.from_pretrained(\"imagetext@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d378e-2033-4ccd-95a8-1f2e5e70c038",
   "metadata": {},
   "source": [
    "Define the Q&A prompt that will be passed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f16782-6804-4ea6-b44b-00ea66c05710",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_prompt = \"does this sink have a dryingboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb510b4-da11-45ed-b082-0f011e366445",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next we will download an image from a url that will be passed to the Imagen Q&A service for inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7dde75-4fbe-4b00-af27-299e501b934a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_url = ('https://mobileimages.lowes.com/productimages/79fc44bb-c9ef-453e-9000-d7cae1372431/49607177.jpg')\n",
    "downloaded_image = 'qa_image.jpg'\n",
    "urllib.request.urlretrieve(image_url, downloaded_image) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f04b7-4c2f-4bba-88a6-a9be5838cc31",
   "metadata": {},
   "source": [
    "Use the Image method from vertexai.vision_model to generate a base64 encoded string of the downloaded image, then delete the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d00c0-63ba-4d45-af98-52dc392631d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_image = Image.load_from_file(location='./'+downloaded_image)\n",
    "os.remove(downloaded_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea0965-13e7-48a9-bc93-eb588e3e0148",
   "metadata": {},
   "source": [
    "Alternatively, you can uncomment the following if you would like to use an exisitng local file instead of downloading an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f7ed9-85c5-4973-a776-cd181d44f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_image = Image.load_from_file(location='./sink1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1980e70-eef9-49e7-a66e-e4c1bff5cd66",
   "metadata": {},
   "source": [
    "We will now create the request body that will be passed to the REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4152412-a31b-4fa2-aa0a-280f5115c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = model.ask_question(\n",
    "    image=source_image,\n",
    "    question=vqa_prompt\n",
    "    # Optional:\n",
    "    #number_of_results=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c44c6c-adcd-4418-b805-8be482304e1e",
   "metadata": {},
   "source": [
    "You can optionally uncomment the following to view the returned status code for verification or troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfcca9-20e4-477d-adfa-3cc66fc3c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e2d0b-8755-48c9-8d64-7a0ab8be5ac7",
   "metadata": {},
   "source": [
    "That's it! Congratulations on defining your first visual Q&A with Imagen!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
