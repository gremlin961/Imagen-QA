{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a331300-5ff3-4219-83c0-ce11703cc9e1",
   "metadata": {},
   "source": [
    "# How To Use Vertex Gemini Vision Pro Generative AI To Inspect Image Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34429b82-078c-4107-827d-8aaed4ab1121",
   "metadata": {},
   "source": [
    "This notebook outlines how to interact with Vertex AI's Gemini Vision Pro GenAI model to inspect images and generate detailed information about its content. Visual Question Answering (VQA) lets you provide an image to the model and ask a question about the image's contents. In response to your question you get one or more natural language answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed54ec-9f36-40af-8d2e-c6fd32a76222",
   "metadata": {},
   "source": [
    "## Prepare the python development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ce888-8c52-476b-96d0-7f65cb259f8d",
   "metadata": {},
   "source": [
    "First, let's identify any project specific variables to customize this notebook to your GCP environment. Change YOUR_PROJECT_ID with your own GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "205088b5-6d5b-46dd-9d77-a8352eac630b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'YOUR_PROJECT_ID'\n",
    "LOCATION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7d4bb-dd35-4477-b94a-8666cf0f9695",
   "metadata": {},
   "source": [
    "Next, let's specify the name of the image file you want to inspect, such as \"OJ.png\" or \"shoe.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ba226a4-d673-4778-8cbd-c32dc88b1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = 'stuff_on_a_shelf.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92738a-045b-48a4-ab0e-1d98fad2089a",
   "metadata": {},
   "source": [
    "Install any needed python modules from our requirements.txt file. Most Vertex Workbench environments include all the packages we'll be using, but if you are using an external Jupyter Notebook or require any additional packages for your own needs, you can simply add them to the included requirements.txt file an run the folloiwng commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f69175b8-63c4-4b5a-837d-055743c0b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b2ce7-0277-4ada-9648-34c67774bfd2",
   "metadata": {},
   "source": [
    "Now we will import all required modules. For our purpose, we will be utilizing the following:\n",
    "\n",
    "- vertexai - Provides authentication access to the Google API's, such as imagegeneration:predict\n",
    "- vertexai.preview.generative_models - Interact with new multimodal models\n",
    "- base64 - Imagen API requests return generated or edited images as base64-encoded strings. This module will help us decode this data to an image file\n",
    "- json - Python module used to interact with JSON data. Imagen returns results in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8e15897-0cb0-4173-9b91-17270a10b989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Part\n",
    "import base64\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b2668-962a-42da-b92b-4932f224bb0d",
   "metadata": {},
   "source": [
    "## Instantiate Vertex AI ojbect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc262a0-fad5-4f15-aee3-cf9ce36a78b0",
   "metadata": {},
   "source": [
    "To use Gemini Vision Pro on Vertex AI you must provide a text description of what you want to inspect, generate or edit. These descriptions are called prompts, and these prompts are the primary way you communicate with Generative AI. Here, we are specifiying what we want the model to identify using a prompt. Play around with this content and see what kind of details you can extract from an image. More information can be found here https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42ad98-e2f9-4c04-a1eb-8f42d2bb1b5e",
   "metadata": {},
   "source": [
    "In this example, we will ask Gemini to inspect a picture of an orange juice carton and provide it's results in a json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94c9ba6f-3c32-4e94-b9ed-779d24db6dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vqa_prompt = 'Briefly describe each product you see in this picture. Provide your response in JSON format including the brand, description, price, size and bin location. If you can not determine the size, mark it as NA. Do not include the json prefix in your response.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40b114-aae7-4cfb-9c20-34bc6773c178",
   "metadata": {},
   "source": [
    "Next we define a function to build the request to be sent to the multimodal model. Two examples are provided, the first creates a base64 encoded string of a local image and uses the from_data function. The second example shows how to provide an image stored in a GCS bucket and use the from_uri function. When using a GCS bucket as the source, you can specify up to 14 different files and do not have to manually create the base64 string. This makes it convenent for processing multiple files at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94b9d834-c641-4ff3-bd90-8b1ad90c5722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(image_filename, \"rb\") as f:\n",
    "    encoded_base_image = base64.b64encode(f.read())\n",
    "B64_BASE_IMAGE = encoded_base_image.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a94e84e-8fd3-418a-9b54-71c43fd34e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(project_id: str, location: str, b64_image: str, prompt: str) -> str:\n",
    "    # Initialize Vertex AI\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    # Load the model\n",
    "    #multimodal_model = GenerativeModel(\"gemini-pro-vision\")\n",
    "    multimodal_model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
    "    # Query the model\n",
    "    response = multimodal_model.generate_content(\n",
    "        [\n",
    "            # Add an example image\n",
    "            Part.from_data(\n",
    "                data=base64.b64decode(b64_image), mime_type=\"image/png\"\n",
    "            ),\n",
    "            #\"what is shown in this image?\",\n",
    "            vqa_prompt,\n",
    "        ]\n",
    "    )\n",
    "    #print(response)\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0bf4c-c3e3-4925-8d49-d4f49c1350d3",
   "metadata": {},
   "source": [
    "Uncomment this section if you would prefer to use an image lcoated in a GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "997bec4d-c8d6-4f90-9726-3e574ab061d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def generate_text(project_id: str, location: str) -> str:\n",
    "#    # Initialize Vertex AI\n",
    "#    vertexai.init(project=project_id, location=location)\n",
    "#    # Load the model\n",
    "#    multimodal_model = GenerativeModel(\"gemini-pro-vision\")\n",
    "#    # Query the model\n",
    "#    response = multimodal_model.generate_content(\n",
    "#        [\n",
    "#            # Add an example image\n",
    "#            Part.from_uri(\n",
    "#                \"gs://generativeai-downloads/images/scones.jpg\", mime_type=\"image/jpeg\"\n",
    "#            ),\n",
    "#            # Add an example query\n",
    "#            vqa_prompt,\n",
    "#        ]\n",
    "#    )\n",
    "#    #print(response)\n",
    "#    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fb974-e836-40c5-81e3-c83f63bc84e0",
   "metadata": {},
   "source": [
    "## Send the request and display the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93462b3-7ee7-4f6f-a141-0d54ce304669",
   "metadata": {},
   "source": [
    "Call the above generate_text fuction and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a32a6c60-9040-4f58-9d25-383e25b86360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'brand': 'Pledge', 'description': 'Expert Care Wood Oil, Amber & Argan Scent', 'price': 10.98, 'size': '14.2 OZ', 'bin_location': '16 57'}, {'brand': 'Old English', 'description': 'Scratch Cover for Light Woods', 'price': 8.28, 'size': '8 OZ', 'bin_location': '16 59'}, {'brand': 'Old English', 'description': 'Scratch Cover for Dark Woods', 'price': 8.28, 'size': '8 OZ', 'bin_location': '16 59'}, {'brand': 'method', 'description': 'Daily Wood Cleaner, Almond', 'price': 6.98, 'size': '28 OZ', 'bin_location': '16 61'}, {'brand': 'Weiman', 'description': 'Leather Conditioning Wipes', 'price': 6.78, 'size': '30 count (7\" x 8\")', 'bin_location': '16 63'}, {'brand': 'Weiman', 'description': 'Leather Conditioner', 'price': 8.98, 'size': '22 FL. OZ.', 'bin_location': '16 65'}, {'brand': 'Resolve', 'description': 'Pet Expert Heavy Traffic Foam Carpet Cleaner', 'price': 22.98, 'size': '22 OZ. (1 LB. 6 OZ.)', 'bin_location': '16 67'}, {'brand': 'Resolve', 'description': 'Easy Clean Brushing Kit', 'price': 'NA', 'size': 'NA', 'bin_location': 'NA'}]\n"
     ]
    }
   ],
   "source": [
    "qa_response = json.loads(generate_text(PROJECT_ID, LOCATION, B64_BASE_IMAGE, vqa_prompt))\n",
    "print(qa_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192001a-47ec-4a6b-a93e-f91ce2e1dd79",
   "metadata": {},
   "source": [
    "That's it! Congratulations on defining your first visual Q&A with Gemini!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
